{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9bc8d79",
   "metadata": {},
   "source": [
    "# üé® Stable Diffusion Backend for Frontend\n",
    "### Google Colab + Cloudflared Tunnel\n",
    "**Optimized for low VRAM GPU**\n",
    "\n",
    "This notebook sets up a complete API backend with memory optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064c402a",
   "metadata": {},
   "source": [
    "## Step 1: Clone server files from GitHub or Upload\n",
    "Option A: Upload server folder to Colab  \n",
    "Option B: Git clone from your repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feaef02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Mount Google Drive and use files from there\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Copy server files from Drive\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Adjust this path to your Drive location\n",
    "# For example: /content/drive/My Drive/Stable_Diffusion/server\n",
    "source_dir = '/content/drive/My Drive/Stable_Diffusion/server'  # CHANGE THIS\n",
    "dest_dir = '/content/server'\n",
    "\n",
    "if os.path.exists(source_dir):\n",
    "    shutil.copytree(source_dir, dest_dir, dirs_exist_ok=True)\n",
    "    print(f'‚úÖ Copied server files from Drive')\n",
    "else:\n",
    "    print(f'‚ö†Ô∏è Source directory not found: {source_dir}')\n",
    "    print('You can upload the server folder manually in Files tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3a6c0e",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f5db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch==2.0.1 torchvision==0.15.2 diffusers==0.21.4 transformers==4.30.2 accelerate==0.20.3 safetensors==0.3.1 flask==2.3.2 flask-cors==4.0.0 pillow==9.5.0 numpy==1.24.3 xformers==0.0.20 peft==0.4.0 requests==2.31.0\n",
    "print('‚úÖ Dependencies installed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5b87fc",
   "metadata": {},
   "source": [
    "## Step 3: Install and Setup Cloudflared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a304f67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Download and install cloudflared\n",
    "!wget -q https://github.com/cloudflare/wrangler/releases/download/wrangler-3.0.1/cloudflared-linux-amd64.deb -O /tmp/cloudflared.deb\n",
    "!dpkg -i /tmp/cloudflared.deb > /dev/null 2>&1\n",
    "\n",
    "# Setup CUDA memory optimization\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "print('‚úÖ Cloudflared installed!')\n",
    "print('‚úÖ CUDA memory optimization enabled!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a68adf",
   "metadata": {},
   "source": [
    "## Step 4: Verify Setup and Start Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f00f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import threading\n",
    "import time\n",
    "import subprocess\n",
    "\n",
    "# Change to server directory\n",
    "os.chdir('/content/server')\n",
    "sys.path.insert(0, '/content/server')\n",
    "\n",
    "print('üìÅ Working directory:', os.getcwd())\n",
    "print('üìÑ Files in directory:')\n",
    "for f in os.listdir('.'):\n",
    "    if not f.startswith('.'):\n",
    "        print(f'  - {f}')\n",
    "\n",
    "# Import config to verify settings\n",
    "try:\n",
    "    import config\n",
    "    print(f'\\n‚úÖ Config loaded')\n",
    "    print(f'  Model: {config.MODEL_ID}')\n",
    "    print(f'  FP16: {config.USE_FP16}')\n",
    "    print(f'  xFormers: {config.ENABLE_XFORMERS}')\n",
    "    print(f'  Attention Slicing: {config.ENABLE_ATTENTION_SLICING}')\n",
    "    print(f'  VAE Tiling: {config.ENABLE_VAE_TILING}')\n",
    "    print(f'  Model CPU Offload: {config.ENABLE_MODEL_CPU_OFFLOAD}')\n",
    "except Exception as e:\n",
    "    print(f'‚ö†Ô∏è Config issue: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c03ace",
   "metadata": {},
   "source": [
    "## Step 5: Start Flask + Cloudflared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7108b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Global variables\n",
    "tunnel_url = None\n",
    "cloudflared_process = None\n",
    "flask_process = None\n",
    "\n",
    "def start_cloudflared():\n",
    "    global tunnel_url, cloudflared_process\n",
    "    \n",
    "    print('üåê Starting cloudflared tunnel...')\n",
    "    try:\n",
    "        cloudflared_process = subprocess.Popen(\n",
    "            ['cloudflared', 'tunnel', '--url', 'http://localhost:5000'],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True,\n",
    "            bufsize=1\n",
    "        )\n",
    "        \n",
    "        # Read output until we find the tunnel URL\n",
    "        timeout = 30\n",
    "        start = time.time()\n",
    "        \n",
    "        while time.time() - start < timeout:\n",
    "            line = cloudflared_process.stdout.readline()\n",
    "            if not line:\n",
    "                time.sleep(0.1)\n",
    "                continue\n",
    "            \n",
    "            print(line.strip())\n",
    "            \n",
    "            # Look for HTTPS URL\n",
    "            match = re.search(r'https://[\\w.-]+\\.trycloudflare\\.com', line)\n",
    "            if match:\n",
    "                tunnel_url = match.group(0)\n",
    "                print(f'\\n‚úÖ Tunnel URL: {tunnel_url}')\n",
    "                print('\\nüìã USE THIS URL IN YOUR FRONTEND SETTINGS!')\n",
    "                break\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'‚ùå Cloudflared error: {e}')\n",
    "\n",
    "def start_flask():\n",
    "    global flask_process\n",
    "    \n",
    "    print('\\nüöÄ Starting Flask server...')\n",
    "    time.sleep(2)  # Give cloudflared time to start\n",
    "    \n",
    "    try:\n",
    "        flask_process = subprocess.Popen(\n",
    "            ['python', 'app.py'],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True,\n",
    "            bufsize=1\n",
    "        )\n",
    "        \n",
    "        # Stream output\n",
    "        for line in flask_process.stdout:\n",
    "            print(line.strip())\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'‚ùå Flask error: {e}')\n",
    "\n",
    "# Start both in threads\n",
    "cloudflared_thread = threading.Thread(target=start_cloudflared, daemon=True)\n",
    "flask_thread = threading.Thread(target=start_flask, daemon=True)\n",
    "\n",
    "cloudflared_thread.start()\n",
    "flask_thread.start()\n",
    "\n",
    "print('‚è≥ Starting servers (this may take 1-2 minutes to load the model)...')\n",
    "print('\\nKeep this cell running - servers are running in background!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b3e6f5",
   "metadata": {},
   "source": [
    "## Step 6: Test API Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2e8446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Wait for server to be ready\n",
    "print('‚è≥ Waiting for server to be ready...')\n",
    "time.sleep(5)\n",
    "\n",
    "# Test local endpoint\n",
    "try:\n",
    "    response = requests.get('http://localhost:5000/api/health', timeout=10)\n",
    "    if response.status_code == 200:\n",
    "        print('‚úÖ Local API is responding')\n",
    "        print(json.dumps(response.json(), indent=2))\n",
    "    else:\n",
    "        print(f'‚ùå Local API error: {response.status_code}')\n",
    "except Exception as e:\n",
    "    print(f'‚è≥ API not ready yet: {e}')\n",
    "    print('Check Step 5 output - model might still be loading')\n",
    "    print('This can take 2-5 minutes on first run')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2afce9e",
   "metadata": {},
   "source": [
    "## Step 7: Monitor Status\n",
    "Run this cell periodically to check memory and generation status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9ab431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "try:\n",
    "    # System info\n",
    "    resp = requests.get('http://localhost:5000/api/system', timeout=5)\n",
    "    print('System Status:')\n",
    "    for key, value in resp.json().items():\n",
    "        if isinstance(value, float):\n",
    "            print(f'  {key}: {value:.2f} GB')\n",
    "        else:\n",
    "            print(f'  {key}: {value}')\n",
    "    \n",
    "    # Memory\n",
    "    print('\\nMemory Usage:')\n",
    "    resp = requests.get('http://localhost:5000/api/memory', timeout=5)\n",
    "    memory = resp.json()\n",
    "    if 'gpu' in memory and memory['gpu']:\n",
    "        gpu = memory['gpu']\n",
    "        print(f\"  GPU Allocated: {gpu['allocated_gb']:.2f} / {gpu['total_gb']:.2f} GB\")\n",
    "        print(f\"  GPU Reserved:  {gpu['reserved_gb']:.2f} GB\")\n",
    "    \n",
    "    # Progress\n",
    "    print('\\nGeneration Status:')\n",
    "    resp = requests.get('http://localhost:5000/api/progress', timeout=5)\n",
    "    progress = resp.json()\n",
    "    print(f\"  Generating: {progress['is_generating']}\")\n",
    "    print(f\"  Current: {progress['current_prompt'][:50] if progress['current_prompt'] else 'None'}...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'‚ùå Error: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2cdd57",
   "metadata": {},
   "source": [
    "## ‚ÑπÔ∏è Usage Instructions\n",
    "\n",
    "### Getting the Public URL:\n",
    "1. Look at Step 5 output - find the line with `https://xxx.trycloudflare.com`\n",
    "2. Copy that URL\n",
    "\n",
    "### In Your Frontend:\n",
    "1. Open your HTML app\n",
    "2. Go to Settings (#settings tab if you have it)\n",
    "3. Paste the Cloudflared URL\n",
    "4. Click \"Test Connection\"\n",
    "5. Start generating!\n",
    "\n",
    "### API Endpoints (POST):\n",
    "- `/api/txt2img` - Text to Image\n",
    "- `/api/img2img` - Image to Image  \n",
    "- `/api/inpaint` - Inpainting\n",
    "\n",
    "### Parameters (JSON):\n",
    "```json\n",
    "{\n",
    "  \"prompt\": \"your description\",\n",
    "  \"negative_prompt\": \"what to avoid\",\n",
    "  \"steps\": 20,\n",
    "  \"cfg_scale\": 7.5,\n",
    "  \"width\": 512,\n",
    "  \"height\": 512,\n",
    "  \"seed\": -1,\n",
    "  \"batch_size\": 1\n",
    "}\n",
    "```\n",
    "\n",
    "### Troubleshooting:\n",
    "- **CUDA out of memory**: Reduce `steps` or `batch_size`\n",
    "- **Slow generation**: Check GPU memory usage in Step 7\n",
    "- **Connection failed**: Check that Cloudflared shows a URL in Step 5\n",
    "- **Server won't start**: Check Step 5 output for errors, model might still be downloading\n",
    "\n",
    "### Tips for Low VRAM:\n",
    "- Use smaller batch sizes (1-2)\n",
    "- Use 20-30 steps instead of 50\n",
    "- The notebook has memory optimization enabled by default\n",
    "- Model runs on GPU with automatic CPU offloading"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
